digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	132340511898304 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	132340508534816 [label=AddmmBackward]
	132340508535104 -> 132340508534816
	132340484514240 [label="fc.bias
 (10)" fillcolor=lightblue]
	132340484514240 -> 132340508535104
	132340508535104 [label=AccumulateGrad]
	132340508535008 -> 132340508534816
	132340508535008 [label=ViewBackward]
	132340508534960 -> 132340508535008
	132340508534960 [label=MeanBackward1]
	132340508535248 -> 132340508534960
	132340508535248 [label=ReluBackward1]
	132340508535344 -> 132340508535248
	132340508535344 [label=AddBackward0]
	132340508535440 -> 132340508535344
	132340508535440 [label=NativeBatchNormBackward]
	132340508535584 -> 132340508535440
	132340508535584 [label=MkldnnConvolutionBackward]
	132340508535776 -> 132340508535584
	132340508535776 [label=ReluBackward1]
	132340508535920 -> 132340508535776
	132340508535920 [label=NativeBatchNormBackward]
	132340508536016 -> 132340508535920
	132340508536016 [label=MkldnnConvolutionBackward]
	132340508535392 -> 132340508536016
	132340508535392 [label=ReluBackward1]
	132340508536304 -> 132340508535392
	132340508536304 [label=AddBackward0]
	132340508536400 -> 132340508536304
	132340508536400 [label=NativeBatchNormBackward]
	132340508536544 -> 132340508536400
	132340508536544 [label=MkldnnConvolutionBackward]
	132340508536736 -> 132340508536544
	132340508536736 [label=ReluBackward1]
	132340508536784 -> 132340508536736
	132340508536784 [label=NativeBatchNormBackward]
	132340484583632 -> 132340508536784
	132340484583632 [label=MkldnnConvolutionBackward]
	132340484583824 -> 132340484583632
	132340484583824 [label=ReluBackward1]
	132340484583968 -> 132340484583824
	132340484583968 [label=AddBackward0]
	132340484584064 -> 132340484583968
	132340484584064 [label=NativeBatchNormBackward]
	132340484584208 -> 132340484584064
	132340484584208 [label=MkldnnConvolutionBackward]
	132340484584400 -> 132340484584208
	132340484584400 [label=ReluBackward1]
	132340484584544 -> 132340484584400
	132340484584544 [label=NativeBatchNormBackward]
	132340484584640 -> 132340484584544
	132340484584640 [label=MkldnnConvolutionBackward]
	132340484584016 -> 132340484584640
	132340484584016 [label=ReluBackward1]
	132340484584928 -> 132340484584016
	132340484584928 [label=AddBackward0]
	132340484585024 -> 132340484584928
	132340484585024 [label=NativeBatchNormBackward]
	132340484585168 -> 132340484585024
	132340484585168 [label=MkldnnConvolutionBackward]
	132340484585360 -> 132340484585168
	132340484585360 [label=ReluBackward1]
	132340484585504 -> 132340484585360
	132340484585504 [label=NativeBatchNormBackward]
	132340484585600 -> 132340484585504
	132340484585600 [label=MkldnnConvolutionBackward]
	132340484585792 -> 132340484585600
	132340484585792 [label=ReluBackward1]
	132340484585936 -> 132340484585792
	132340484585936 [label=AddBackward0]
	132340484586032 -> 132340484585936
	132340484586032 [label=NativeBatchNormBackward]
	132340484586176 -> 132340484586032
	132340484586176 [label=MkldnnConvolutionBackward]
	132340484586368 -> 132340484586176
	132340484586368 [label=ReluBackward1]
	132340484586512 -> 132340484586368
	132340484586512 [label=NativeBatchNormBackward]
	132340484586608 -> 132340484586512
	132340484586608 [label=MkldnnConvolutionBackward]
	132340484585984 -> 132340484586608
	132340484585984 [label=ReluBackward1]
	132340484586896 -> 132340484585984
	132340484586896 [label=AddBackward0]
	132340484586992 -> 132340484586896
	132340484586992 [label=NativeBatchNormBackward]
	132340484587136 -> 132340484586992
	132340484587136 [label=MkldnnConvolutionBackward]
	132340484587328 -> 132340484587136
	132340484587328 [label=ReluBackward1]
	132340484587472 -> 132340484587328
	132340484587472 [label=NativeBatchNormBackward]
	132340484587376 -> 132340484587472
	132340484587376 [label=MkldnnConvolutionBackward]
	132340484600112 -> 132340484587376
	132340484600112 [label=ReluBackward1]
	132340484600256 -> 132340484600112
	132340484600256 [label=AddBackward0]
	132340484600352 -> 132340484600256
	132340484600352 [label=NativeBatchNormBackward]
	132340484600496 -> 132340484600352
	132340484600496 [label=MkldnnConvolutionBackward]
	132340484600688 -> 132340484600496
	132340484600688 [label=ReluBackward1]
	132340484600832 -> 132340484600688
	132340484600832 [label=NativeBatchNormBackward]
	132340484600928 -> 132340484600832
	132340484600928 [label=MkldnnConvolutionBackward]
	132340484600304 -> 132340484600928
	132340484600304 [label=ReluBackward1]
	132340484601216 -> 132340484600304
	132340484601216 [label=AddBackward0]
	132340484601312 -> 132340484601216
	132340484601312 [label=NativeBatchNormBackward]
	132340484601456 -> 132340484601312
	132340484601456 [label=MkldnnConvolutionBackward]
	132340484601648 -> 132340484601456
	132340484601648 [label=ReluBackward1]
	132340484601792 -> 132340484601648
	132340484601792 [label=NativeBatchNormBackward]
	132340484601888 -> 132340484601792
	132340484601888 [label=MkldnnConvolutionBackward]
	132340484601264 -> 132340484601888
	132340484601264 [label=MaxPool2DWithIndicesBackward]
	132340484602176 -> 132340484601264
	132340484602176 [label=ReluBackward1]
	132340484602272 -> 132340484602176
	132340484602272 [label=NativeBatchNormBackward]
	132340484602368 -> 132340484602272
	132340484602368 [label=MkldnnConvolutionBackward]
	132340484602560 -> 132340484602368
	132340511873344 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	132340511873344 -> 132340484602560
	132340484602560 [label=AccumulateGrad]
	132340484602320 -> 132340484602272
	132340508796736 [label="bn1.weight
 (64)" fillcolor=lightblue]
	132340508796736 -> 132340484602320
	132340484602320 [label=AccumulateGrad]
	132340484601984 -> 132340484602272
	132340508798720 [label="bn1.bias
 (64)" fillcolor=lightblue]
	132340508798720 -> 132340484601984
	132340484601984 [label=AccumulateGrad]
	132340484602080 -> 132340484601888
	132340508834240 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	132340508834240 -> 132340484602080
	132340484602080 [label=AccumulateGrad]
	132340484601840 -> 132340484601792
	132340508835008 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	132340508835008 -> 132340484601840
	132340484601840 [label=AccumulateGrad]
	132340484601696 -> 132340484601792
	132340508834048 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	132340508834048 -> 132340484601696
	132340484601696 [label=AccumulateGrad]
	132340484601600 -> 132340484601456
	132340508835776 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	132340508835776 -> 132340484601600
	132340484601600 [label=AccumulateGrad]
	132340484601408 -> 132340484601312
	132340511888512 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	132340511888512 -> 132340484601408
	132340484601408 [label=AccumulateGrad]
	132340484601360 -> 132340484601312
	132340508389440 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	132340508389440 -> 132340484601360
	132340484601360 [label=AccumulateGrad]
	132340484601264 -> 132340484601216
	132340484601120 -> 132340484600928
	132340508390080 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	132340508390080 -> 132340484601120
	132340484601120 [label=AccumulateGrad]
	132340484600880 -> 132340484600832
	132340508390272 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	132340508390272 -> 132340484600880
	132340484600880 [label=AccumulateGrad]
	132340484600736 -> 132340484600832
	132340508390464 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	132340508390464 -> 132340484600736
	132340484600736 [label=AccumulateGrad]
	132340484600640 -> 132340484600496
	132340508391104 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	132340508391104 -> 132340484600640
	132340484600640 [label=AccumulateGrad]
	132340484600448 -> 132340484600352
	132340508390400 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	132340508390400 -> 132340484600448
	132340484600448 [label=AccumulateGrad]
	132340484600400 -> 132340484600352
	132340508391168 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	132340508391168 -> 132340484600400
	132340484600400 [label=AccumulateGrad]
	132340484600304 -> 132340484600256
	132340484600064 -> 132340484587376
	132340508393216 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	132340508393216 -> 132340484600064
	132340484600064 [label=AccumulateGrad]
	132340484599920 -> 132340484587472
	132340508446848 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	132340508446848 -> 132340484599920
	132340484599920 [label=AccumulateGrad]
	132340484599872 -> 132340484587472
	132340508446912 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	132340508446912 -> 132340484599872
	132340484599872 [label=AccumulateGrad]
	132340484587280 -> 132340484587136
	132340508447552 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	132340508447552 -> 132340484587280
	132340484587280 [label=AccumulateGrad]
	132340484587088 -> 132340484586992
	132340508447488 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	132340508447488 -> 132340484587088
	132340484587088 [label=AccumulateGrad]
	132340484587040 -> 132340484586992
	132340508447616 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	132340508447616 -> 132340484587040
	132340484587040 [label=AccumulateGrad]
	132340484586944 -> 132340484586896
	132340484586944 [label=NativeBatchNormBackward]
	132340484587424 -> 132340484586944
	132340484587424 [label=MkldnnConvolutionBackward]
	132340484600112 -> 132340484587424
	132340484600160 -> 132340484587424
	132340508392256 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	132340508392256 -> 132340484600160
	132340484600160 [label=AccumulateGrad]
	132340484587232 -> 132340484586944
	132340508392320 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	132340508392320 -> 132340484587232
	132340484587232 [label=AccumulateGrad]
	132340484587184 -> 132340484586944
	132340508392576 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	132340508392576 -> 132340484587184
	132340484587184 [label=AccumulateGrad]
	132340484586800 -> 132340484586608
	132340508448256 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	132340508448256 -> 132340484586800
	132340484586800 [label=AccumulateGrad]
	132340484586560 -> 132340484586512
	132340508448448 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	132340508448448 -> 132340484586560
	132340484586560 [label=AccumulateGrad]
	132340484586416 -> 132340484586512
	132340508448640 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	132340508448640 -> 132340484586416
	132340484586416 [label=AccumulateGrad]
	132340484586320 -> 132340484586176
	132340508449536 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	132340508449536 -> 132340484586320
	132340484586320 [label=AccumulateGrad]
	132340484586128 -> 132340484586032
	132340508448960 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	132340508448960 -> 132340484586128
	132340484586128 [label=AccumulateGrad]
	132340484586080 -> 132340484586032
	132340508449600 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	132340508449600 -> 132340484586080
	132340484586080 [label=AccumulateGrad]
	132340484585984 -> 132340484585936
	132340484585744 -> 132340484585600
	132340508509696 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	132340508509696 -> 132340484585744
	132340484585744 [label=AccumulateGrad]
	132340484585552 -> 132340484585504
	132340508509888 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	132340508509888 -> 132340484585552
	132340484585552 [label=AccumulateGrad]
	132340484585408 -> 132340484585504
	132340508510080 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	132340508510080 -> 132340484585408
	132340484585408 [label=AccumulateGrad]
	132340484585312 -> 132340484585168
	132340508511040 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	132340508511040 -> 132340484585312
	132340484585312 [label=AccumulateGrad]
	132340484585120 -> 132340484585024
	132340508510592 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	132340508510592 -> 132340484585120
	132340484585120 [label=AccumulateGrad]
	132340484585072 -> 132340484585024
	132340508511104 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	132340508511104 -> 132340484585072
	132340484585072 [label=AccumulateGrad]
	132340484584976 -> 132340484584928
	132340484584976 [label=NativeBatchNormBackward]
	132340484585696 -> 132340484584976
	132340484585696 [label=MkldnnConvolutionBackward]
	132340484585792 -> 132340484585696
	132340484585840 -> 132340484585696
	132340508508352 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	132340508508352 -> 132340484585840
	132340484585840 [label=AccumulateGrad]
	132340484585264 -> 132340484584976
	132340508508416 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	132340508508416 -> 132340484585264
	132340484585264 [label=AccumulateGrad]
	132340484585216 -> 132340484584976
	132340508508672 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	132340508508672 -> 132340484585216
	132340484585216 [label=AccumulateGrad]
	132340484584832 -> 132340484584640
	132340508512064 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	132340508512064 -> 132340484584832
	132340484584832 [label=AccumulateGrad]
	132340484584592 -> 132340484584544
	132340508557376 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	132340508557376 -> 132340484584592
	132340484584592 [label=AccumulateGrad]
	132340484584448 -> 132340484584544
	132340508557568 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	132340508557568 -> 132340484584448
	132340484584448 [label=AccumulateGrad]
	132340484584352 -> 132340484584208
	132340508558528 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	132340508558528 -> 132340484584352
	132340484584352 [label=AccumulateGrad]
	132340484584160 -> 132340484584064
	132340508558080 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	132340508558080 -> 132340484584160
	132340484584160 [label=AccumulateGrad]
	132340484584112 -> 132340484584064
	132340508558592 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	132340508558592 -> 132340484584112
	132340484584112 [label=AccumulateGrad]
	132340484584016 -> 132340484583968
	132340484583776 -> 132340484583632
	132340508561280 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	132340508561280 -> 132340484583776
	132340484583776 [label=AccumulateGrad]
	132340484583584 -> 132340508536784
	132340508598400 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	132340508598400 -> 132340484583584
	132340484583584 [label=AccumulateGrad]
	132340484583488 -> 132340508536784
	132340508598592 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	132340508598592 -> 132340484583488
	132340484583488 [label=AccumulateGrad]
	132340508536688 -> 132340508536544
	132340508599552 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	132340508599552 -> 132340508536688
	132340508536688 [label=AccumulateGrad]
	132340508536496 -> 132340508536400
	132340508599104 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	132340508599104 -> 132340508536496
	132340508536496 [label=AccumulateGrad]
	132340508536448 -> 132340508536400
	132340508599616 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	132340508599616 -> 132340508536448
	132340508536448 [label=AccumulateGrad]
	132340508536352 -> 132340508536304
	132340508536352 [label=NativeBatchNormBackward]
	132340508536640 -> 132340508536352
	132340508536640 [label=MkldnnConvolutionBackward]
	132340484583824 -> 132340508536640
	132340484583872 -> 132340508536640
	132340508559936 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	132340508559936 -> 132340484583872
	132340484583872 [label=AccumulateGrad]
	132340508536592 -> 132340508536352
	132340508560000 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	132340508560000 -> 132340508536592
	132340508536592 [label=AccumulateGrad]
	132340484583728 -> 132340508536352
	132340508560256 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	132340508560256 -> 132340484583728
	132340484583728 [label=AccumulateGrad]
	132340508536208 -> 132340508536016
	132340508600576 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	132340508600576 -> 132340508536208
	132340508536208 [label=AccumulateGrad]
	132340508535968 -> 132340508535920
	132340508600768 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	132340508600768 -> 132340508535968
	132340508535968 [label=AccumulateGrad]
	132340508535824 -> 132340508535920
	132340508600960 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	132340508600960 -> 132340508535824
	132340508535824 [label=AccumulateGrad]
	132340508535728 -> 132340508535584
	132340508601920 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	132340508601920 -> 132340508535728
	132340508535728 [label=AccumulateGrad]
	132340508535536 -> 132340508535440
	132340508601472 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	132340508601472 -> 132340508535536
	132340508535536 [label=AccumulateGrad]
	132340508535488 -> 132340508535440
	132340508601984 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	132340508601984 -> 132340508535488
	132340508535488 [label=AccumulateGrad]
	132340508535392 -> 132340508535344
	132340508535056 -> 132340508534816
	132340508535056 [label=TBackward]
	132340508535296 -> 132340508535056
	132340508796224 [label="fc.weight
 (10, 512)" fillcolor=lightblue]
	132340508796224 -> 132340508535296
	132340508535296 [label=AccumulateGrad]
	132340508534816 -> 132340511898304
}
